{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhagesh20558/miniconda3/envs/mhcp3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.hfDataset import MHCoPilot_Dataset \n",
    "from generation_data_prepro1 import _add_spans2posts, _generate_llama2_inputs, sample_test_adder, generate_category_classifier_train_data, generate_category_classifier_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3331/3331 [00:01<00:00, 2669.44 examples/s]\n",
      "Map: 100%|██████████| 953/953 [00:00<00:00, 3047.54 examples/s]\n",
      "Map: 100%|██████████| 476/476 [00:00<00:00, 2863.08 examples/s]\n",
      "Map: 100%|██████████| 3331/3331 [00:02<00:00, 1240.48 examples/s]\n",
      "Map: 100%|██████████| 953/953 [00:00<00:00, 1357.57 examples/s]\n",
      "Map: 100%|██████████| 476/476 [00:00<00:00, 1343.67 examples/s]\n",
      "Map: 100%|██████████| 3331/3331 [00:06<00:00, 496.46 examples/s]\n",
      "Map: 100%|██████████| 3331/3331 [00:13<00:00, 246.57 examples/s]\n",
      "Map: 100%|██████████| 3331/3331 [00:02<00:00, 1313.01 examples/s]\n",
      "Map: 100%|██████████| 953/953 [00:01<00:00, 507.93 examples/s]\n",
      "Map: 100%|██████████| 953/953 [00:03<00:00, 254.91 examples/s]\n",
      "Map: 100%|██████████| 953/953 [00:00<00:00, 1391.97 examples/s]\n",
      "Map: 100%|██████████| 476/476 [00:00<00:00, 500.77 examples/s]\n",
      "Map: 100%|██████████| 476/476 [00:02<00:00, 219.64 examples/s]\n",
      "Map: 100%|██████████| 476/476 [00:00<00:00, 1250.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"../data\"\n",
    "data_task =  \"general\"\n",
    "data_ner = False\n",
    "make_new_split = True\n",
    "\n",
    "Dataset = MHCoPilot_Dataset(data_path, task = data_task, ner = data_ner, make_new_split = make_new_split)\n",
    "Dataset.get_data()\n",
    "Dataset.train_df = Dataset.train_df.map(sample_test_adder)\n",
    "Dataset.train_df = Dataset.train_df.map(_add_spans2posts)\n",
    "Dataset.train_df = Dataset.train_df.map(_generate_llama2_inputs)\n",
    "\n",
    "Dataset.val_df = Dataset.val_df.map(sample_test_adder)\n",
    "Dataset.val_df = Dataset.val_df.map(_add_spans2posts)\n",
    "Dataset.val_df = Dataset.val_df.map(_generate_llama2_inputs)\n",
    "\n",
    "Dataset.test_df = Dataset.test_df.map(sample_test_adder)\n",
    "Dataset.test_df = Dataset.test_df.map(_add_spans2posts)\n",
    "Dataset.test_df = Dataset.test_df.map(_generate_llama2_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Been adicited to opiates (oxy) consistently for 3 years and want stop. Please help.',\n",
       " 'body': \"Hello, I'm a 24 year old college student in Northern CA getting ready to graduate with my BS after next semester and I am addicted to oxycodone. I've never said this out loud before but I believe myself to be a functional opiate addict with no prior drug related addictions or mental health problems. I  was intoduced to painkillers (norcos) in 2013 when I got surgery on my torn ACL and was prescribed a (60 count I believe) bottle which I refilled once. Since then I have been taking opiates on and off for the last 6 years. I've really been taking opiates consistently for the last 2 years though, at this point I've grown dependent to oxycodone (taking on avg. between 2-5, 30mg pills a day) and have tried to stop on my own but within the last few months, but have unable to stop. I'm on this forum today asking for advice on what I should do. Everytime I've tried to stop on my own I get horrible withdrawals and always seem to come baeck for more. I know I have so much life ahead of me and I really want to be able to graduate and get a job without having to live with this addiction. Even when I am able to go a couple of days without taking anything, oxy is all that I can think about and I become extremely depressed, anxious, and crave more. I'm double insured through Kaiser and Sutter and have been heavily considering going to Kaiser for treatment but I am worried about what will happen if I do. I work and am in college full time so I don't want to be checked into an inpatient rehab center which is what's held me back for so long from going to Kaiser for help. Also, not wanting my family to find out about this situation is extremely important to me. I've recently talked to my friend's mom who has been prescribed Suboxone for a while now and she said it helped her beat her opiate addiction. She's even given me some before which really helped a lot with withdrawals and killed my urge to take more opiates but only taking a couple Suboxone after being on oxy for so long didn't help with the mental part of it. Once I ran out of the Suboxone I was able to go a few days without taking anything until I started feeling bad again and the need to take opiates came back into my mind, which then is when it all started up again. Overall, I'm completely mentally and physically exhausted from taking opiates and slowly killing myself so if anybody has any advice on how I should go about getting help or how the process works at Kaiser then please let me know. I'm scared that this problem I have is going to ruin my life if I don't get ahold of it before it's too late. I personally think that getting a prescription for suboxone would really do wonders for me because it really did help me when I was on them. At this point, any advice or words of wisdom from people who have experienced what I'm going through is greatly appreciated. Thank you for your time.\",\n",
       " 'annotated_post_body': \"<es>Hello, I'm a 24 year old college student in Northern CA getting ready to graduate with my BS after next semester and I am addicted to oxycodone.<ee> <es>I've never said this out loud before but I believe myself to be a functional opiate addict with no prior drug related addictions or mental health problems.<ee> <es>I  was intoduced to painkillers (norcos) in 2013 when I got surgery on my torn ACL and was prescribed a (60 count I believe) bottle which I refilled once.<ee> <es>Since then I have been taking opiates on and off for the last 6 years.<ee> <es>I've really been taking opiates consistently for the last 2 years though, at this point I've grown dependent to oxycodone (taking on avg. between 2-5, 30mg pills a day) and have tried to stop on my own but within the last few months, but have unable to stop.<ee> <rsI'm on this forum today asking for advice on what I should do.<re> <efs>Everytime I've tried to stop on my own I get horrible withdrawals and always seem to come baeck for more.<efe> <rs>I know I have so much life ahead of me and I really want to be able to graduate and get a job without having to live with this addiction.<re> <efs>Even when I am able to go a couple of days without taking anything, oxy is all that I can think about and I become extremely depressed, anxious, and crave more.<efe> <es>I'm double insured through Kaiser and Sutter and have been heavily considering going to Kaiser for treatment but I am worried about what will happen if I do.<ee> <es>I work and am in college full time so I don't want to be checked into an inpatient rehab center which is what's held me back for so long from going to Kaiser for help.<ee> <es>Also, not wanting my family to find out about this situation is extremely important to me.<ee> <es>I've recently talked to my friend's mom who has been prescribed Suboxone for a while now and she said it helped her beat her opiate addiction.<ee> <es>She's even given me some before which really helped a lot with withdrawals and killed my urge to take more opiates but only taking a couple Suboxone after being on oxy for so long didn't help with the mental part of it.<ee> <efs>Once I ran out of the Suboxone I was able to go a few days without taking anything until I started feeling bad again and the need to take opiates came back into my mind, which then is when it all started up again.<efe> <efs>Overall, I'm completely mentally and physically exhausted from taking opiates and slowly killing myself.<efe> <rs>so if anybody has any advice on how I should go about getting help or how the process works at Kaiser then please let me know.<re> <efs>I'm scared that this problem I have is going to ruin my life if I don't get ahold of it before it's too late.<efe> <rs>I personally think that getting a prescription for suboxone would really do wonders for me because it really did help me when I was on them.<re> <rs>At this point, any advice or words of wisdom from people who have experienced what I'm going through is greatly appreciated.<re> Thank you for your time.\",\n",
       " 'ES': 2,\n",
       " 'EFS': 2,\n",
       " 'RS': 2,\n",
       " 'EMaskingQ': '',\n",
       " 'EMask': '',\n",
       " 'EFSMaskingQ': '',\n",
       " 'EFSMask': '',\n",
       " 'RMaskingQ': '',\n",
       " 'RMask': '',\n",
       " '__index_level_0__': 4051,\n",
       " 'labels': [2.0, 2.0, 2.0],\n",
       " 'es_indices': [0,\n",
       "  153,\n",
       "  317,\n",
       "  480,\n",
       "  559,\n",
       "  1329,\n",
       "  1495,\n",
       "  1671,\n",
       "  1770,\n",
       "  1921,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'ee_indices': [144,\n",
       "  308,\n",
       "  471,\n",
       "  550,\n",
       "  817,\n",
       "  1486,\n",
       "  1662,\n",
       "  1761,\n",
       "  1912,\n",
       "  2140,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'efs_indices': [896,\n",
       "  1158,\n",
       "  2149,\n",
       "  2373,\n",
       "  2623,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'efe_indices': [1001,\n",
       "  1318,\n",
       "  2362,\n",
       "  2477,\n",
       "  2732,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'rs_indices': [1012,\n",
       "  2488,\n",
       "  2743,\n",
       "  2892,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 're_indices': [887,\n",
       "  1149,\n",
       "  2614,\n",
       "  2883,\n",
       "  3016,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_es_indices': [0,\n",
       "  153,\n",
       "  317,\n",
       "  480,\n",
       "  559,\n",
       "  1329,\n",
       "  1495,\n",
       "  1671,\n",
       "  1770,\n",
       "  1921,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_ee_indices': [144,\n",
       "  308,\n",
       "  471,\n",
       "  550,\n",
       "  817,\n",
       "  1486,\n",
       "  1662,\n",
       "  1761,\n",
       "  1912,\n",
       "  2140,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_efs_indices': [896,\n",
       "  1158,\n",
       "  2149,\n",
       "  2373,\n",
       "  2623,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_efe_indices': [1001,\n",
       "  1318,\n",
       "  2362,\n",
       "  2477,\n",
       "  2732,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_rs_indices': [1012,\n",
       "  2488,\n",
       "  2743,\n",
       "  2892,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_re_indices': [887,\n",
       "  1149,\n",
       "  2614,\n",
       "  2883,\n",
       "  3016,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1],\n",
       " 'pred_spans_body': \"<es>Hello, I'm a 24 year old college student in Northern CA getting ready to graduate with my BS after next semester and I am addicted to oxycodone.<ee> <es>I've never said this out loud before but I believe myself to be a functional opiate addict with no prior drug related addictions or mental health problems.<ee> <es>I  was intoduced to painkillers (norcos) in 2013 when I got surgery on my torn ACL and was prescribed a (60 count I believe) bottle which I refilled once.<ee> <es>Since then I have been taking opiates on and off for the last 6 years.<ee> <es>I've really been taking opiates consistently for the last 2 years though, at this point I've grown dependent to oxycodone (taking on avg. between 2-5, 30mg pills a day) and have tried to stop on my own but within the last few months, but have unable to stop.<ee> I'm on this forum today asking for advice on what I should do. Everyti<efs>me I've tried to stop on my own I get horrible withdrawals and always seem to come baeck for more. I know<efe> <rs><re>. Everyti<efs>me I've tried to stop on my own I get horrible withdrawals and always seem to come baeck for more. I know<efe> I have so muc<efs>h life ahead of me and I really want to be able to graduate and get a job without having to live with this addiction. Even when I am able to go a couple of days<efe> <es>without taking anything, oxy is all that I can think about and I become extremely depressed, anxious, and crave more. I'm double insured through Kaiser and S<ee>u<es>tter and have been heavily considering going to Kaiser for treatment but I am worried about what will happen if I do. I work and am in college full time so I don't wan<ee>t<es> to be checked into an inpatient rehab center which is what's held me back for so long fro<ee>m<es> going to Kaiser for help. Also, not wanting my family to find out about this situation is extremely important to me. I've recently talked to <ee>m<es>y friend's mom who has been prescribed Suboxone for a while now and she said it helped her beat her opiate addiction. She's even given me some before which really helped a lot with withdrawals and killed my urge to take<ee> <efs>more opiates but only taking a couple Suboxone after being on oxy for so long didn't help with the mental part of it. Once I ran out of the Suboxone I was able to go a few days without taking anything until I star<efe>t<efs>ed feeling bad again and the need to take opiates came back into my mind, which then is when it all star<efe>t<rs><re>ve so muc<efs>h life ahead of me and I really want to be able to graduate and get a job without having to live with this addict<efs>ion. Even when I am able to go a couple of days<efe> <es>without taking anything, oxy is all that I can think<efe> <rs><re>is addict<efs>ion. Even when I am able to go a couple of days<efe> <es>without taking anything, oxy is all that I can think<efe> about and I <rs><re>ut and I become extremely depressed, anxious, and crave more. I'm double insured through Kaiser and S<ee>u<es>tter and have been heavily considering going to Kaiser for treatment but I am worried about what will happen if I do. I work and am in college full time so I don't wan<ee>t<es> to be checked into an inpatient rehab center which is what's held me back for so long fro<ee>m<es> going to Kaiser for help. Also, not wanting my family to find out about this situation is extremely important to me. I've recently talked to <ee>m<es>y friend's mom who has been prescribed Suboxone for a while now and she said it helped her beat her opiate addiction. She's even given me some before which really helped a lot with withdrawals and killed my urge to take<ee> <efs>more opiates but only taking a couple Suboxone after being on oxy for so long didn't help with the mental part of it. Once I ran out of the Suboxone I was able to go a few days without taking anything until I star<efe>t<efs>ed feeling bad again and the need to take opiates came back into my mind, which then is when it all star<efe>ted up again. Overall, I'm completely mentally and physically exhausted from taking opiates and slowly killing myself so if anybody has any advice on how I should go about getting help or how the process works at Kaiser then please let me know. I'm scared that this problem I have is going to ruin my life if I don't get ahold of it before it's too late. I personally think that getting a prescription for suboxone would really do wonders for me because it really did help me when I was on them. At this point, any advice or words of wisdom from people who have experienced what I'm going through is greatly appreciated. Thank you for your time.\",\n",
       " 'input_prompt': '<s>[INST] A support seeker on a peer-to-peer (P2P) Online Mental Health Platform (OMHP) is an individual who utilizes digital services to seek assistance/help for managing and improving their mental health, typically through interactions with peer groups or self-help resources.\\n\\n    A support provider in a peer-to-peer (P2P) context on an Online Mental Health Platform (OMHP) is an individual who offers guidance, emotional support, and shared experiences to support seekers, leveraging their own insights and understanding of mental health challenges to help others in similar situations.\\n\\n    An ideal help-seeking post on Online Mental Health Platforms like Reddit, TalkLife, etc. comprises three fundamental parameters, namely event, effect and requirement. They dictate the help-seeking nature of posts. These parameters are crucial in crafting posts that resonate with support providers, facilitating a more profound understanding and relevant responses. The parameters are defined as follows:\\n    Event: This parameter encapsulates the specific situation, activity, or event that is the focal point of the support seeker’s concern. The explicit detailing of such events provides a contextual background essential for empathetic understanding.\\n    Effect: This aspect targets the impact or consequences of the identified event on the support seeker. By elucidating the effect, the post conveys the emotional or practical repercussions of the event, thereby inviting more targeted and empathetic responses. \\n    Requirement: This parameter is critical in directing the nature of the assistance sought. It ranges from emotional and informational support to instrumental aid, thereby guiding the potential response trajectory. This highlights the importance of clearly articulated needs for effective support.\\n\\n    In the posts on OMHP these parameters can have intensity ranging from 0 to 2, where 0 means absent, 1 means present but needs clarification and 2 being well described based on the presence of these parameters in the post\\n\\n    Consider the following post by a support seeker on a OMHP, in which the spans of text representing Event, Effect and Requirement have been marked. Also, the intensity levels for each of the parameters in the post have been provided along with the post.\\n\\n    The post <es> and <ee> tags encapsulate the spans for the Event parameter, <efs> and <efe> tags encapsulate the spans for the Effect parameter, and <rs> and <re> tags encapsulate the spans for the Requirement parameter.\\n\\n    [POST]<es>Hello, I\\'m a 24 year old college student in Northern CA getting ready to graduate with my BS after next semester and I am addicted to oxycodone.<ee> <es>I\\'ve never said this out loud before but I believe myself to be a functional opiate addict with no prior drug related addictions or mental health problems.<ee> <es>I  was intoduced to painkillers (norcos) in 2013 when I got surgery on my torn ACL and was prescribed a (60 count I believe) bottle which I refilled once.<ee> <es>Since then I have been taking opiates on and off for the last 6 years.<ee> <es>I\\'ve really been taking opiates consistently for the last 2 years though, at this point I\\'ve grown dependent to oxycodone (taking on avg. between 2-5, 30mg pills a day) and have tried to stop on my own but within the last few months, but have unable to stop.<ee> I\\'m on this forum today asking for advice on what I should do. Everyti<efs>me I\\'ve tried to stop on my own I get horrible withdrawals and always seem to come baeck for more. I know<efe> <rs><re>. Everyti<efs>me I\\'ve tried to stop on my own I get horrible withdrawals and always seem to come baeck for more. I know<efe> I have so muc<efs>h life ahead of me and I really want to be able to graduate and get a job without having to live with this addiction. Even when I am able to go a couple of days<efe> <es>without taking anything, oxy is all that I can think about and I become extremely depressed, anxious, and crave more. I\\'m double insured through Kaiser and S<ee>u<es>tter and have been heavily considering going to Kaiser for treatment but I am worried about what will happen if I do. I work and am in college full time so I don\\'t wan<ee>t<es> to be checked into an inpatient rehab center which is what\\'s held me back for so long fro<ee>m<es> going to Kaiser for help. Also, not wanting my family to find out about this situation is extremely important to me. I\\'ve recently talked to <ee>m<es>y friend\\'s mom who has been prescribed Suboxone for a while now and she said it helped her beat her opiate addiction. She\\'s even given me some before which really helped a lot with withdrawals and killed my urge to take<ee> <efs>more opiates but only taking a couple Suboxone after being on oxy for so long didn\\'t help with the mental part of it. Once I ran out of the Suboxone I was able to go a few days without taking anything until I star<efe>t<efs>ed feeling bad again and the need to take opiates came back into my mind, which then is when it all star<efe>t<rs><re>ve so muc<efs>h life ahead of me and I really want to be able to graduate and get a job without having to live with this addict<efs>ion. Even when I am able to go a couple of days<efe> <es>without taking anything, oxy is all that I can think<efe> <rs><re>is addict<efs>ion. Even when I am able to go a couple of days<efe> <es>without taking anything, oxy is all that I can think<efe> about and I <rs><re>ut and I become extremely depressed, anxious, and crave more. I\\'m double insured through Kaiser and S<ee>u<es>tter and have been heavily considering going to Kaiser for treatment but I am worried about what will happen if I do. I work and am in college full time so I don\\'t wan<ee>t<es> to be checked into an inpatient rehab center which is what\\'s held me back for so long fro<ee>m<es> going to Kaiser for help. Also, not wanting my family to find out about this situation is extremely important to me. I\\'ve recently talked to <ee>m<es>y friend\\'s mom who has been prescribed Suboxone for a while now and she said it helped her beat her opiate addiction. She\\'s even given me some before which really helped a lot with withdrawals and killed my urge to take<ee> <efs>more opiates but only taking a couple Suboxone after being on oxy for so long didn\\'t help with the mental part of it. Once I ran out of the Suboxone I was able to go a few days without taking anything until I star<efe>t<efs>ed feeling bad again and the need to take opiates came back into my mind, which then is when it all star<efe>ted up again. Overall, I\\'m completely mentally and physically exhausted from taking opiates and slowly killing myself so if anybody has any advice on how I should go about getting help or how the process works at Kaiser then please let me know. I\\'m scared that this problem I have is going to ruin my life if I don\\'t get ahold of it before it\\'s too late. I personally think that getting a prescription for suboxone would really do wonders for me because it really did help me when I was on them. At this point, any advice or words of wisdom from people who have experienced what I\\'m going through is greatly appreciated. Thank you for your time.[/POST]\\n    [INTENSITY]Event Intensity: 2, Effect Intensity: 2, Requirement Intensity: 2[/INTENSITY]\\n\\n    Based on the above post and intensity levels generate one question for each of the parameters to ask the support seeker to prompt them to improve the post content to make it more help-seeking to facilitate a more profound understanding and relevant responses from support providers.\\n\\n    Format for Response: \"<EQ>Question on Event</EQ> <EFQ>Question on Effect</EFQ> <RQ>Question on Requirement</RQ>\"\\n\\n    Note: Generate at max 1 question for each of the parameters. Strictly follow the format provided for responses. [/INST] <EQ></EQ> <EFQ></EFQ> <RQ></RQ> </s>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.train_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3331/3331 [00:02<00:00, 1255.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "Dataset.train_df = Dataset.train_df.map(generate_category_classifier_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_data = generate_category_classifier_trainset(Dataset.train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 9993/9993 [00:00<00:00, 42757.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "cc_train_data = cc_train_data.filter(lambda example: example['question'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'labels'],\n",
       "    num_rows: 3890\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 953/953 [00:00<00:00, 1402.82 examples/s]\n",
      "Filter: 100%|██████████| 2859/2859 [00:00<00:00, 41079.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'labels'],\n",
       "    num_rows: 1110\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.val_df = Dataset.val_df.map(generate_category_classifier_train_data)\n",
    "cc_val_data = generate_category_classifier_trainset(Dataset.val_df)\n",
    "cc_val_data = cc_val_data.filter(lambda example: example['question'] != '')\n",
    "cc_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/476 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 476/476 [00:00<00:00, 1310.74 examples/s]\n",
      "Filter: 100%|██████████| 1428/1428 [00:00<00:00, 37676.71 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'labels'],\n",
       "    num_rows: 560\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.test_df = Dataset.test_df.map(generate_category_classifier_train_data)\n",
    "cc_test_data = generate_category_classifier_trainset(Dataset.test_df)\n",
    "cc_test_data = cc_test_data.filter(lambda example: example['question'] != '')\n",
    "cc_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3890/3890 [00:03<00:00, 1149.96 examples/s]\n",
      "Map: 100%|██████████| 1110/1110 [00:00<00:00, 1554.03 examples/s]\n",
      "Map: 100%|██████████| 560/560 [00:00<00:00, 1098.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "cc_train_data_tokenized = cc_train_data.map(tokenize_function, batched=True)\n",
    "cc_val_data_tokenized = cc_val_data.map(tokenize_function, batched=True)\n",
    "cc_test_data_tokenized = cc_test_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhagesh20558/miniconda3/envs/mhcp3/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_cc',       \n",
    "    num_train_epochs=2,             \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=16,  \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs_cc',           \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     \n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=cc_train_data_tokenized,\n",
    "    eval_dataset=cc_val_data_tokenized,\n",
    "    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaran21258\u001b[0m (\u001b[33mkaran912\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bhagesh20558/MH-CoPilot/Pipeline/wandb/run-20240724_030802-p9jtgr1q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karan912/huggingface/runs/p9jtgr1q' target=\"_blank\">./results_cc</a></strong> to <a href='https://wandb.ai/karan912/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karan912/huggingface' target=\"_blank\">https://wandb.ai/karan912/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karan912/huggingface/runs/p9jtgr1q' target=\"_blank\">https://wandb.ai/karan912/huggingface/runs/p9jtgr1q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='974' max='974' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [974/974 05:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.153314</td>\n",
       "      <td>0.967568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.066118</td>\n",
       "      <td>0.987387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=974, training_loss=0.20546543101820275, metrics={'train_runtime': 341.1788, 'train_samples_per_second': 22.803, 'train_steps_per_second': 2.855, 'total_flos': 2047022389923840.0, 'train_loss': 0.20546543101820275, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06611756980419159, 'eval_accuracy': 0.9873873873873874, 'eval_runtime': 13.0782, 'eval_samples_per_second': 84.874, 'eval_steps_per_second': 5.352, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9839285714285714\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(cc_test_data_tokenized)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./cc_saved_model/tokenizer_config.json',\n",
       " './cc_saved_model/special_tokens_map.json',\n",
       " './cc_saved_model/vocab.json',\n",
       " './cc_saved_model/merges.txt',\n",
       " './cc_saved_model/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./cc_saved_model')\n",
    "tokenizer.save_pretrained('./cc_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained('./cc_saved_model')\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained('./cc_saved_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhcp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
